With the first batch after the arrival ChatGPT yet to graduate this year it 
“Going through College is just proportional to how well you use ChatGPT”, remarked a third-year student
A fitting reflection of what the state of affairs has become with the advent of AI’s rapid evolution over the past four years. Its impact, while shocking from the start, was less useful in the beginning as other than the occasional use to generate some poems, write an essay or improve grammar, it unreliable but with time, they have matured into instruments of direct utility, from doing assignments, writing code, SOPs, and even making PPTs in minutes.




This swift shift in academic culture has raised concerns about the erosion of critical thinking as as delegating cognitive task to these tools may lead to reduction of mental effort required for genuine reflection. This hypothesis while still a matter of research has already been supported by various studies.

While some people might also like to believe that this is a corruption in intellectual thought and morally exploitative of the nature of work in academia, they forget the simple truth about its utility and the Incentive structure around it.

More people than one would like to admit are here in IIT to find careers to sustain them, and most tend to find them in non-core fields. Thus, from very early in their stat at IITB they realise that the biggest outcome from academics is the CGPA they can gain. This creates a classic Principal-agent problem: where “principal” (professor/IIT) wants learning and the “agent” (student) wants a good grade with minimal effort. Thus, the pressure due to these incentives and the seductive, easy utility of AI tools makes students lean towards them as learning takes a backseat.

While most professors cannot do much about the underlying issue, they still want students to learn more. In the chase of this just cause, a significant portion of professors tend to follow the “American way” and ban the use of any AI tools stating it in their introduction class itself, and still keep giving out assignments.In this case, the only incentive to not use any LLM is the consequence of them getting caught. While this approach has mostly been appropriate for all historical “cheating tools”, LLM output is much harder to catch.

“ The tools right now are not sharp enough to reliably catch anyone. It has happened that sometimes what I have written has been said to be LLM output by such tools” Alok Takkar, Prof at Ashoka

While copying assignments is not new and it used to happen even before the advent of LLMs, right now, due to the level of AI slop, it has become much harder to distinguish between their quality. This has led professors to de-emphasise assignments, supplementing them with vivas or cutting their weight in the final grade, to the level that they just become a study material provided by professors for students to work out if they are interested.
However, this is not ideal as Assignments turn concepts into practice, letting students apply ideas, get feedback, and deepen understanding, and professors don’t want to let go of such an important learning tool.

“We’re opening on-campus Ashoka labs with no LLMs or a restricted-scaffolded LLM that acts as a TA, so students can complete assignments onsite with supervised AI support.” -Alok Takkar
While an interesting idea, Alok himself admits that students could still memorise the LLM's output and type it in labs, but he still thinks that it will create enough deterrence. 

The LLM's Problems are not constrained to classrooms. The Labs' many professors head and work at have also started using these tools in different ways, leading to their own complexities. At the KCDH department, Professor Saket adopts a notably liberal stance toward AI us,e where students are free to rely on these tools, provided they can fully explain the code they submit. The same openness once extended to his research lab, until, while discussing with his PhD students, otherwise a top performer, froze when he asked him to code something simple in front of him. Reflecting on this incident, he remarked
“As first-year PhDs are still learning them, relying on such tools mindlessly hinders the development of some fundamental skills”
The elaborate language capabilities of these tools have created many other problems, as Saket recalls how he receives a lot of resumes and research proposals with elaborate and exciting ideas, but when he gets to meet those behind those documents, he is rather disappointed with their lack of knowledge 

Optimistic trends 
That being said, LLMs and other generative AI tools have largely led to a positive impact on research. Specific AI models with agentic capabilities are used by professors to do a significant proportion of grunt code work, analysis of unstructured data, etc. This speeds up their process by weeks, giving them more time for multiple iterations and experiments. This is especially true in computational research, where the time of prototyping and testing has come down significantly 

Many professors have also started using these tools to improve their teaching. Professor Viraj from IISC Bangalore believes these tools challenge the old teaching rules, particularly the ones championing “Learning by doing”. He takes a CS101 equivalent course in IISC and has transformed his teaching in light of these new tools.
He does not believe in writing code to learn as the best method; instead, according to him, learning to review and critique the code is more important, thus his assignments involve students trying to find bugs in a given code. He also thinks that understanding the assumptions behind a problem and asking the right question to clarify them is also a really important skill; thus, his assignments also involve asking the right clarifying question, which is something LLMs are lacking in.
Along those lines Prof. Vinay Kukarni, who teaches DS203, allows the use of these tools in assignments and even exams. He himself uses tools like NotebookLM to consolidate student resources by feeding student assignments to gauge insights about them. He remarks 
“It is rather unfair to expect the students not to use the same tools when I use them myself, and they will also use them when they graduate"
Meanwhile, In a recent workshop on AI in design, industry professionals, professors, and students collaborated on a project that would normally span an entire semester but was condensed into a single day. Despite the tight timeline, teams produced outcomes that can be graded around a C to B–, reflecting commendable effort given the constraints.
 Most professors, while not this radical, have also found interesting ways to use LLMs in their teaching. prof Umakant, who teaches various Ent courses, created a GPT-based chatbot to help students while researching for case studies and assignments. While some professors have taken a more critical approach, asking students to generate math proof through LLMs, and then correct or debug it. while on the sidelines, using LLMS to improve their own teaching 
“Once, when I wanted to explain a lengthy derivation to my class, I sought help from AI tools like ChatGPT and Perplexity. They provided various approaches to the derivation, which proved to be very helpful.”
Student use of LLMs is also not just entirely negative. These tools have led to a richer democratisation of information aligning with the original intent of the internet. The personalized guidance capabilities of LLMs capable of explaining concepts, generating examples, or walking them through complex derivations step by step have enabled students to gain a deeper understanding. This has empowered students to depend less on professors using the given resource and their favorite LLM to navigate through many courses. 
This creates a dichotomy between the using LLMs to learn and offloading intellectual tasks. As these tools are still emerging and it is tough to deterministically say what is the correct way to use them, however, most professors believe relying on AI tools without original thought or deep understanding will not lead to sustainable success. Most also conceding that the syntax for writing the code will take a back seat and the mathematical and algorithmic understanding will matter. Similarly, using LLMs to compensate for English or other language skill is unanimously accepted as long as the content is original. 
In fields like design, it has become imperative to have languages and prompting skills to use generative AI 
“Understanding language now enables the production of good designs. AI has effectively democratized the design process, allowing those who previously could not draw or imagine themselves as designers to participate” - hod design
Industry demands familiarity with these tools, yet students often produce obvious and subpar outputs due to a lack of language, prompting, and judgment skills. Therefore, a curriculum change is necessary.

IITB policy future  
OpEd on plagiarism definition and possible policy(how not to enter data into llm)
ETHICS 
“Disruptions have happened before,each simplifying our workflows but this one is unprecedented, it is taking creativity and thought away from us” - phani

LLMs influence is not limited to learning the subconscious offloading of even basic thoughts and emotions have enabled LLMs dictate what to wear , where to eat , navigating chat scenarios.This kind of advice which was earlier sourced from friends leading to meaningful social interactions has now been outsourced to LLMs. This adds into the already fueled fire of social isolation, creating a vicious loop when these very LLMs contributing to the problem start offering chat products to ‘solve’ it. A famous example is Anie by Grok, a companion video-based chat system designed for socially isolated individuals.Though some products like Anie feel more like a superficial attempt, AI is also increasingly being explored as a genuine tool for mental health support.

Beyond concerns about the social and intellectual impact of LLMs, the ethical and moral implications of their very use are a subject of ongoing debate.
Prof Phani, from the IDC department like many others, believes that the process to create the art matters more than the final output as you may create a 3D figure but you will not see what a sculptor sees in his creation. Every technology in the past like 3d printing , shift from paper to digital animation etc we have gone one step away from the “art process” with the current wave of GenAI producing “art” on whims being the biggest disconnect.
“Society develops through art as a reflection of human experience, Taking art away from humanity and giving it to machines could creatively handicap entire generations”

His concerns are more than just moral arguments; he shares the belief that AI artwork is often an amalgamation of previous works without proper credit or compensation to original artists. Whether this classifies as plagiarism is a matter in court around the world. Nevertheless, there's a strong belief that governments should establish copyright and compensation policies, which institutions such as IITB would then enforce. Without such regulations, there's a risk that AI could diminish the value of art, artists, and the entire cultural process of creation.

But not everyone shares this opinion. Prof Venkatesh Rajamanickam also from IDC 
Believes
“When AI generates a Ghibli-style image or writes poetry that rivals the work of celebrated authors, it is not merely mimicking -- it is reflecting the cumulative knowledge, creativity, and expression of humanity”
 These generative models are the culmination of decades of research, mathematics, and the dedicated work of thousands of researchers and engineers who explored language, cognition, and creativity. According to him dismissing the output of these systems as mere simulacra devoid of meaning is to ignore the awe-inspiring journey that led to their creation.
This debate whether the outputs of generativeAI is soulless and mechanical due to its lack of human touch and artistic process or it is as meaningful as any art and simply different, reflecting the zeitgeist of the digital age is one of the defining question for the future of human creativity.

Even in research AI is raising similar plagiarism debate, specifically if AI should be credited as author 
 “if you properly site and credit things then there is no problem,I can submit a paper as long as I admit that LLMs have helped me write it in a significant way” - s Sudarshan. 
This along with other data security concerns have led many big publications and research institutes to draft policies,For instance, an IISc committee warned its research community about risks associated with using AI tools. These risks include the potential for sensitive content entered into AI tools to be used for further model training (tantamount to public disclosure), and the unreliability of AI-generated content. Authors are responsible for the accuracy of any such content included in their publications.
The concerns about data security and privacy are paramount as some professors commit to not putting sensitive student data in LLMs while other not aware about the scale. Recent defence and even sensitive Private research projects have started adding clauses to protect their data from getting leaked into LLMs. Thus an overarching AI policy is needed to address such concerns. 
