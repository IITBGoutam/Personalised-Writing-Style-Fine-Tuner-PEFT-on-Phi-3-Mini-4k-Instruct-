[STYLE_SAMPLE] With the first student cohort to use ChatGPT throughout their college careers yet to graduate, the tool's full academic impact remains an open question.
[STYLE_SAMPLE] Although its impact was immediate, the tool's initial utility was limited; once unreliable and used for trivial tasks like generating poems or improving grammar, it has since matured into an instrument of direct academic utility, capable of completing assignments, writing code, SOPs, and even generating presentations in minutes.
[STYLE_SAMPLE] This swift shift in academic culture has raised concerns about the potential erosion of critical thinking, as delegating cognitive tasks to AI may reduce the mental effort required for genuine reflection.
[STYLE_SAMPLE] This hypothesis, while still a subject of ongoing research, has already found support in various academic studies.
[STYLE_SAMPLE] While some observers frame this trend as a corruption of intellectual thought or as morally exploitative of academia, this view often overlooks the tools' practical utility and the powerful incentive structures driving their adoption.
[STYLE_SAMPLE] A significant portion of students at IIT attend the institute primarily to secure sustainable careers, many of which are ultimately found in non-core fields.
[STYLE_SAMPLE] Consequently, students often realize early in their time at IITB that one of the most significant outcomes of their academic efforts is the final CGPA they achieve.
[STYLE_SAMPLE] This dynamic creates a classic principal-agent problem, wherein the "principal"—the professor or institution—desires genuine learning, while the "agent"—the student—is incentivized to achieve a good grade with minimal effort.
[STYLE_SAMPLE] The pressure from these incentives, combined with the accessible utility of AI tools, compels students to adopt them, often causing the goal of genuine learning to take a backseat.
[STYLE_SAMPLE] Although most professors feel they cannot resolve this underlying incentive problem, they remain committed to fostering deeper student learning.
[STYLE_SAMPLE] In pursuit of this goal, a significant portion of professors have opted to ban the use of AI tools—often announcing this policy in their introductory class—while continuing to assign traditional homework.
[STYLE_SAMPLE] Under such a policy, the primary incentive for a student to avoid using an LLM becomes the fear of being caught.
[STYLE_SAMPLE] While this deterrent-based approach may have been effective against historical "cheating tools," output from modern LLMs is significantly harder to detect.
[STYLE_SAMPLE] “The tools right now are not sharp enough to reliably catch anyone.
[STYLE_SAMPLE] It has happened that sometimes what I have written has been said to be LLM output by such tools,” remarked Alok Takkar, a Professor at Ashoka University.
[STYLE_SAMPLE] This challenge has led some professors to de-emphasize assignments by reducing their weight in the final grade or supplementing them with oral examinations (vivas), effectively turning the assignments into optional study materials.
[STYLE_SAMPLE] This trend is not ideal, as assignments are a critical tool for turning theory into practice, allowing students to apply concepts, receive feedback, and deepen their understanding—a pedagogical tool professors are reluctant to abandon.
[STYLE_SAMPLE] “We’re opening on-campus Ashoka labs with no LLMs or a restricted-scaffolded LLM that acts as a TA, so students can complete assignments onsite with supervised AI support.
[STYLE_SAMPLE] ” -Alok Takkar. While an interesting proposal, he admits students could still memorize an LLM's output and type it in labs, though he believes it will create enough deterrence.
[STYLE_SAMPLE] The challenges posed by LLMs are not constrained to the classroom.
[STYLE_SAMPLE] Research labs where many professors work have also begun using these tools, leading to a new set of complexities.
[STYLE_SAMPLE] At the KCDH department, for example, Professor Saket has adopted a notably liberal stance on AI use, permitting students to rely on these tools provided they can fully explain the code they submit.
[STYLE_SAMPLE] This same openness once extended to his research lab, until a top-performing PhD student froze when asked to write a simple piece of code during a discussion.
[STYLE_SAMPLE] Reflecting on this, he remarked, “As first-year PhDs are still learning... relying on such tools mindlessly hinders the development of some fundamental skills.”
[STYLE_SAMPLE] Professors are now using specific AI models with agentic capabilities to perform a significant proportion of "grunt work," such as writing boilerplate code and analyzing unstructured data.
[STYLE_SAMPLE] This acceleration speeds up their research process by weeks, giving them more time for multiple iterations and experiments.
[STYLE_SAMPLE] This is especially true in computational research, where prototyping and testing timelines have been reduced significantly.
[STYLE_SAMPLE] Professor Viraj from IISC Bangalore believes these tools fundamentally challenge traditional teaching methodologies, particularly the emphasis on "learning by doing."
[STYLE_SAMPLE] He teaches a CS101-equivalent course at IISC and has completely transformed his pedagogy in light of these new tools.
[STYLE_SAMPLE] He no longer believes that writing code from scratch is the optimal learning method.
[STYLE_SAMPLE] Instead, according to him, learning to review and critique code is more important; consequently, his assignments now involve students trying to find bugs in a given code.
[STYLE_SAMPLE] He also emphasizes that understanding a problem's underlying assumptions and asking clarifying questions are critical skills.
[STYLE_SAMPLE] Thus, his assignments require students to formulate the right questions, a task where LLMs currently lack proficiency.
[STYLE_SAMPLE] Along those lines, Prof. Vinay Kukarni, who teaches DS203, allows the use of these tools in assignments and even exams.
[STYLE_SAMPLE] He utilizes tools like NotebookLM to consolidate resources and analyze student assignments to gauge class-wide understanding.
[STYLE_SAMPLE] “It is rather unfair to expect the students not to use the same tools when I use them myself, and they will also use them when they graduate," he remarked.
[STYLE_SAMPLE] Despite the compressed timeline, the teams produced outcomes graded in the C to B- range, reflecting a commendable effort given the constraints.
[STYLE_SAMPLE] Most professors, while not adopting such radical changes, have also found innovative ways to integrate LLMs into their teaching.
[STYLE_SAMPLE] Professor Umakant, who teaches entrepreneurship courses, created a custom GPT-based chatbot to assist students researching case studies and assignments.
[STYLE_SAMPLE] Some professors have adopted a more critical approach, tasking students with generating mathematical proofs using LLMs and then requiring them to correct or debug the output.
[STYLE_SAMPLE] Meanwhile, many are simultaneously using LLMs on the sidelines to refine their own teaching methods.
[STYLE_SAMPLE] “Once, when I wanted to explain a lengthy derivation to my class, I sought help from AI tools like ChatGPT and Perplexity.
[STYLE_SAMPLE] They provided various approaches to the derivation, which proved to be very helpful.”
[STYLE_SAMPLE] These tools have fostered a richer democratization of information, aligning with the internet's original intent.
[STYLE_SAMPLE] The personalized guidance capabilities of LLMs—which can explain concepts, generate examples, or provide step-by-step walkthroughs of complex derivations—have enabled students to gain a deeper understanding of course material.
[STYLE_SAMPLE] This has empowered students to become less dependent on professors, instead using provided resources in tandem with their preferred LLM to navigate their courses.
[STYLE_SAMPLE] This reality creates a dichotomy between using LLMs as a learning aid and merely offloading intellectual tasks to them.
[STYLE_SAMPLE] As these tools are still emerging, it is difficult to determine a single "correct" way to use them; however, most professors believe that relying on AI without original thought will not lead to sustainable success.
[STYLE_SAMPLE] Most also concede that rote memorization of coding syntax will take a backseat to a deeper mathematical and algorithmic understanding.
[STYLE_SAMPLE] Similarly, using LLMs to compensate for language or writing skills is almost unanimously accepted, provided the underlying content and ideas are original.
[STYLE_SAMPLE] In fields like design, language proficiency and prompting skills have become imperative for effectively using generative AI.
[STYLE_SAMPLE] “Understanding language now enables the production of good designs.
[STYLE_SAMPLE] AI has effectively democratized the design process, allowing those who previously could not draw or imagine themselves as designers to participate,” noted the Head of the Design department.
[STYLE_SAMPLE] Therefore, many argue, a curriculum change is necessary.
[STYLE_SAMPLE] This category of advice, once sourced from friends during meaningful social interactions, is now increasingly being outsourced to LLMs.
[STYLE_SAMPLE] This trend exacerbates existing social isolation, creating a vicious loop wherein the same LLMs contributing to the problem are marketed as a solution via companion chat products.
[STYLE_SAMPLE] A prominent example is "Anie" by Grok, a video-based companion chat system designed specifically for socially isolated individuals.
[STYLE_SAMPLE] While some of these products appear to be superficial attempts, AI is also being seriously explored as a genuine tool for mental health support.
[STYLE_SAMPLE] Beyond the social and intellectual impacts, the fundamental ethical and moral implications of using LLMs remain a subject of intense debate.
[STYLE_SAMPLE] Professor Phani of the IDC department, like many others, believes the creative process matters more than the final output; one may generate a 3D figure, but they will not experience the perspective of the sculptor who created it.
[STYLE_SAMPLE] He argues that past technologies, such as 3D printing and the shift to digital animation, moved artists one step away from the "art process," but the current wave of generative AI represents the most significant disconnect thus far.
[STYLE_SAMPLE] “Society develops through art as a reflection of human experience. Taking art away from humanity and giving it to machines could creatively handicap entire generations.”
[STYLE_SAMPLE] His concerns are more than just moral arguments; he shares the belief that AI-generated artwork is often an amalgamation of existing works, created without proper credit or compensation to the original artists.
[STYLE_SAMPLE] Whether this practice legally constitutes plagiarism is currently being litigated in courts around the world.
[STYLE_SAMPLE] Nevertheless, there is a strong contingent that believes governments must establish clear copyright and compensation policies, which institutions like IITB could then enforce.
[STYLE_SAMPLE] Without such regulations, there is a significant risk that AI could diminish the value of art, artists, and the cultural creation process itself.
[STYLE_SAMPLE] However, not everyone shares this critical opinion.
[STYLE_SAMPLE] Professor Venkatesh Rajamanickam, also from IDC, holds a different view.
[STYLE_SAMPLE] He believes, “When AI generates a Ghibli-style image or writes poetry that rivals the work of celebrated authors, it is not merely mimicking -- it is reflecting the cumulative knowledge, creativity, and expression of humanity.”
[STYLE_SAMPLE] According to Professor Rajamanickam, dismissing these outputs as "mere simulacra" is to ignore the awe-inspiring scientific journey that led to their creation.
[STYLE_SAMPLE] This debate—whether generative AI output is soulless and mechanical, or if it is a new, meaningful art form reflecting the digital zeitgeist—is one of the defining questions for the future of human creativity.
[STYLE_SAMPLE] In research, AI is raising a similar plagiarism debate, particularly over whether an AI should be credited as an author. “If you properly cite and credit things, then there is no problem," said Professor S. Sudarshan. "I can submit a paper as long as I admit that LLMs have helped me write it in a significant way.”
[STYLE_SAMPLE] This issue, along with broader data security concerns, has led major publications and research institutes to draft new policies; an IISc committee, for instance, recently warned its research community about the risks of using AI tools.
[STYLE_SAMPLE] These risks include the potential for sensitive data entered into AI tools to be used for model training—which is tantamount to public disclosure—and the inherent unreliability of AI-generated content.
[STYLE_SAMPLE] Authors are ultimately responsible for the accuracy of any such content included in their publications.
[STYLE_SAMPLE] Concerns about data security and privacy are paramount, as some professors actively avoid entering sensitive student data into LLMs, while others may not be aware of the risks.
[STYLE_SAMPLE] Recent defense and sensitive private-sector research projects have begun adding clauses to contracts to protect their data from being leaked via LLMs.
[STYLE_SAMPLE] Thus, many argue, an overarching institutional AI policy is needed to address these burgeoning concerns.